{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation de données \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing des reviews \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "    # Pour filtrer la langue des avis\n",
    "from langdetect import detect, DetectorFactory \n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "\n",
    "\n",
    "# Modelisation\n",
    "    # Méthode classique \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import hdbscan\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "    # Méthode avancée\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from transformers import BertTokenizer, BertModel, TFBertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bertopic import BERTopic \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pour enregistrer et charger les modèles\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARGEMENT DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation d'un dataframe vide\n",
    "df = pd.read_csv('Data/avis_ugc_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de passer nos données dans les modèles, nous devons les mettre en forme. Tout d’abord, j’ai constaté que la colonne `language` contient souvent des informations incorrectes sur la langue. Pour remédier à cela, nous allons utiliser la librairie `langdetect`, qui s’appuie sur la détection de langue de Google pour identifier la langue correcte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df_clean = df.copy()\"\"\"\n",
    "\n",
    "# Importation du fichier déjà nettoyé car le code est long à tourner\n",
    "df_clean = pd.read_csv('Data/avis_ugc_fr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DetectorFactory.seed = 24\n",
    "\n",
    "# Fonction pour détecter la langue d'un texte donné\n",
    "def detect_language(text: str) -> str | None:\n",
    "    try:\n",
    "        # Tente de détecter la langue du texte\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        # En cas d'erreur (par exemple, si le texte est vide ou ambigu), retourne None\n",
    "        return None\n",
    "\n",
    "# Applique la fonction de détection de langue à chaque avis dans la colonne 'review'\n",
    "df_clean['language_detected'] = df_clean['review'].map(detect_language)\n",
    "\n",
    "# Filtre les données pour ne conserver que les avis détectés en français ('fr')\n",
    "df_clean = df[df['language_detected'] == 'fr']\n",
    "\n",
    "# Exporte le DataFrame filtré dans un fichier CSV\n",
    "df_clean.to_csv('Data/avis_ugc_fr.csv', index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Nombre d'avis non francais supprimés : {df.shape[0] - df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des stop words français de la librairie NLTK\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Ajout de stop words détectés dans les avis \n",
    "custom_stop_words = {\",\", \".\", \"a\", \"c'est\", \"!\", \"film\", \"cinéma\", \n",
    "                     \"si\", \"plus\", \"ugc\", \"ca\", \"là\", \"où\", \"deux\", \n",
    "                     \"ça\", \"h\", \"qu\", \"il\", \"''\", '``', \"-\", \"(\", \")\",\n",
    "                     \"..\", \"€\", \"?\", \"....\", \"//\", \":\", \"/\", \";\", '’'\n",
    "                    }\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "# Initialisation d'un lemmatiseur pour réduire les mots à leur forme de base\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_reviews(reviews: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cette fonction prend une série pandas contenant des avis textuels et applique un prétraitement\n",
    "    pour chaque avis. Elle effectue les étapes suivantes :\n",
    "    \n",
    "    1. Convertit chaque avis en minuscules pour uniformiser le texte.\n",
    "    2. Supprime les chiffres et remplace les apostrophes par des espaces pour éviter les séparations incorrectes.\n",
    "    3. Tokenise chaque avis, c'est-à-dire qu'elle divise le texte en mots individuels.\n",
    "    4. Supprime les stop words (mots courants sans importance) à partir d'une liste de stop words standard en français,\n",
    "       enrichie de mots spécifiques au contexte.\n",
    "    5. Applique la lemmatisation, qui réduit les mots à leur forme de base (par exemple, \"films\" devient \"film\").\n",
    "    \n",
    "    Paramètres :\n",
    "    reviews (pd.Series) : Série pandas contenant les avis textuels.\n",
    "    \n",
    "    Retourne :\n",
    "    pd.Series : Série pandas contenant des listes de mots prétraités pour chaque avis.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Convertir en minuscules et supprimer les chiffres et les apostrophes\n",
    "    reviews = reviews.str.lower().str.replace(r'\\d+', '', regex=True).str.replace(\"'\", ' ')\n",
    "\n",
    "    # Appliquer tokenisation, suppression des stop words, et lemmatisation\n",
    "    reviews = reviews.apply(lambda review: [lemmatizer.lemmatize(word) \n",
    "                                            for word in word_tokenize(review) \n",
    "                                            if word not in stop_words\n",
    "                                           ])\n",
    "\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = preprocess_reviews(df_clean['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_word_cloud(reviews: pd.Series, title: str = None, subplot: tuple[int] = None, background_image_path: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Génère et affiche un nuage de mots basé sur le contenu textuel d'une série pandas contenant des avis.\n",
    "    La fonction peut également afficher le nuage de mots sur une image de fond spécifiée.\n",
    "    \n",
    "    Paramètres :\n",
    "    - reviews (pd.Series) : Série pandas contenant les avis textuels. Tous les avis sont concaténés pour créer le nuage de mots.\n",
    "    - title (str, optionnel) : Titre à afficher au-dessus du nuage de mots. Si None, aucun titre n'est affiché.\n",
    "    - subplot (tuple[int], optionnel) : Spécifie l'emplacement du nuage de mots dans une grille de sous-plots sous la forme (nrows, ncols, index).\n",
    "      Utilisé lorsque le nuage de mots doit être affiché dans une figure avec plusieurs sous-graphiques.\n",
    "    - background_image_path (str, optionnel) : Chemin d'accès à une image de fond pour le nuage de mots.\n",
    "      Si fourni, le nuage de mots prendra la forme de cette image. Si None, le fond sera blanc par défaut.\n",
    "    \n",
    "    Retourne :\n",
    "    - None : La fonction affiche directement le nuage de mots en utilisant `matplotlib.pyplot` et ne retourne rien.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatène tous les avis en une seule chaîne de texte\n",
    "    text = ' '.join(reviews.astype(str))\n",
    "    \n",
    "    # Charge l'image de fond si spécifiée, sinon crée un nuage de mots avec un fond blanc\n",
    "    if background_image_path is not None:\n",
    "        background_image = np.array(Image.open(background_image_path))\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', mask=background_image, contour_width=1, contour_color='black').generate(text)\n",
    "    else:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Positionne le nuage de mots dans un sous-plot si `subplot` est spécifié\n",
    "    if subplot is not None :\n",
    "        plt.subplot(subplot[0], subplot[1], subplot[2])\n",
    "    \n",
    "    # Affiche le nuage de mots et enlève les axes\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ajoute un titre si spécifié\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_cloud(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle pour afficher un word cloud pour chaque note\n",
    "for i in np.sort(df_clean['rating'].unique()):\n",
    "    reviews_per_note =  preprocess_reviews(df_clean[df_clean['rating'] == i]['review'])\n",
    "    \n",
    "    title = f' Word Cloud des avis {i} étoiles'\n",
    "    my_word_cloud(reviews_per_note, title, subplot=(3,2,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le mot \"salle(s)\" apparaît aussi bien dans les avis positifs que négatifs, ce qui suggère qu'il n'est pas un indicateur fiable de sentiment. Il serait donc pertinent de le retirer, au moins pour la création du nuage de mots, afin d'obtenir une visualisation plus représentative des termes associés aux avis positifs ou négatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle pour afficher un word cloud pour chaque note\n",
    "for i in np.sort(df_clean['rating'].unique()):\n",
    "    reviews_per_note =  preprocess_reviews(df_clean[df_clean['rating'] == i]['review'])\n",
    "    \n",
    "    reviews_per_note = reviews_per_note.apply(lambda words: [word for word in words if word not in ['salle', 'salles']])\n",
    "    \n",
    "    title = f' Word Cloud des avis {i} étoiles'\n",
    "    my_word_cloud(reviews_per_note, title, subplot=(3,2,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Word Cloud des avis 1 étoile :**\n",
    "\n",
    "    * Les mots les plus fréquents incluent \"place\", \"séance\", et \"bien\". Cela pourrait suggérer des commentaires sur l'emplacement, l'organisation des séances, ou même des aspects de confort. Cependant, dans un contexte de note basse, ces termes sont probablement utilisés de manière négative (ex. : mauvaise organisation des séances, inconfort des places).\n",
    "    * Les termes \"dommage\", \"problème\", et \"mal\" apparaissent également, ce qui renforce l'idée de critiques.\n",
    "\n",
    "2. **Word Cloud des avis 2 étoiles :**\n",
    "\n",
    "    * On retrouve des mots similaires comme \"place\" et \"tout\". \"Dommage\" est également présent, ce qui indique des critiques mitigées.\n",
    "    * Le mot \"bien\" reste fréquent, mais dans ce contexte, il pourrait être utilisé pour souligner des aspects corrects dans un avis globalement négatif.\n",
    "\n",
    "3. **Word Cloud des avis 3 étoiles :**\n",
    "\n",
    "    * Les termes \"bien\" et \"peu\" sont dominants, suggérant des avis modérés où des aspects positifs sont soulignés, mais avec des réserves (ex. : \"peu d'espace\", \"bien mais pourrait être mieux\").\n",
    "    * D’autres mots comme \"dommage\", \"prix\", et \"petite\" indiquent probablement des points d'amélioration signalés par les utilisateurs.\n",
    "\n",
    "4. **Word Cloud des avis 4 étoiles :**\n",
    "\n",
    "    * Le mot \"bien\" devient encore plus dominant, accompagné de \"très\", \"bon\", et \"propre\". Cela suggère des avis majoritairement positifs, où la qualité et le confort sont reconnus.\n",
    "    * Des termes comme \"agréable\" et \"choix\" apparaissent, ce qui peut indiquer une satisfaction avec les options disponibles et l’atmosphère générale.\n",
    "\n",
    "5. **Word Cloud des avis 5 étoiles :**\n",
    "\n",
    "    * Les mots les plus fréquents sont \"très\", \"super\", \"bien\", et \"agréable\", soulignant une satisfaction élevée.\n",
    "    * On voit aussi \"parfait\", \"personnel\", et \"choix\", ce qui indique une expérience globalement positive avec de nombreux aspects appréciés, y compris le service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELISATION : Identifier les différentes thématiques des avis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de méthode classique (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des avis en une liste de texte\n",
    "reviews = [' '.join(review) for review in reviews]\n",
    "\n",
    "# On applique la transformation TF-IDF aux avis pour obtenir une matrice sparse des scores TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DBSCAN identifie des clusters en regroupant les points qui se trouvent dans des zones de densité uniforme, ce qui limite sa capacité pour des données à densité variable.\n",
    ">\n",
    "> HDBSCAN est une version améliorée qui détecte les clusters de densité variable de manière hiérarchique, permettant une identification plus flexible et automatique des groupes dans des ensembles de données complexes. HDBSCAN n’a pas besoin de définir `eps` et se base principalement sur deux paramètres : `min_cluster_size` (taille minimale pour un cluster) et `min_samples` (contrôle de la sensibilité au bruit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_hdbscan = PCA(n_components=100) \n",
    "reduced_vectors = pca_hdbscan.fit_transform(X)\n",
    "\n",
    "sum(pca_hdbscan.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=3)\n",
    "hdbscan_model.fit(reduced_vectors)\"\"\"\n",
    "\n",
    "hdbscan_model = joblib.load('Entrainement_modeles_tfidf/hdbscan_tfidf.joblib')\n",
    "\n",
    "# Résultats des clusters\n",
    "labels_hdbscan = hdbscan_model.labels_\n",
    "\n",
    "calinski_score = calinski_harabasz_score(reduced_vectors, labels_hdbscan)\n",
    "print(\"Score de Calinski-Harabasz :\", calinski_score) \n",
    "\n",
    "\"\"\"joblib.dump(hdbscan_model, 'Entrainement_modeles_tfidf/hdbscan_tfidf.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['HDBSCAN_labels'] = labels_hdbscan\n",
    "\n",
    "df_clean.groupby('HDBSCAN_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                rating_Count=('rating', 'count')\n",
    "                               ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du score de silhouette pour déterminer le nombre optimal de clusters\n",
    "\"\"\"silhouette_scores = []\n",
    "range_n_clusters = range(5, 20)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=24)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print('nombre clusters :', n_clusters, silhouette_avg)\n",
    "\n",
    "# Tracer le score de silhouette pour chaque nombre de clusters\n",
    "plt.plot(range_n_clusters, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Method\")\n",
    "plt.xlabel(\"Nombre de clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()\"\"\"\n",
    "\n",
    "# Choisir le nombre optimal de clusters\n",
    "\"\"\"optimal_clusters = range_n_clusters[np.argmax(silhouette_scores)]\"\"\"\n",
    "\n",
    "optimal_clusters = joblib.load('Entrainement_modeles_tfidf/optimal_clusters_kmean_tfidf.joblib')\n",
    "\n",
    "\"\"\"joblib.dump(optimal_clusters, 'Entrainement_modeles_tfidf/optimal_clusters_kmean_tfidf.joblib')\"\"\"\n",
    "\n",
    "print(f\"Nombre de clusters optimal basé sur le score de silhouette: {optimal_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=24)\n",
    "\n",
    "df_clean['Kmean_labels'] = kmeans.fit_predict(X)\n",
    "\n",
    "df_clean.groupby('Kmean_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                rating_Count=('rating', 'count')\n",
    "                               ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Je n'ai pas pu implémenter cet algorithme en raison de problèmes de mémoire que je n'ai pas réussi à résoudr.CHA attend un vecteur dense ce qui consomme beaucoup trop de mémoire. Je partage néanmoins le code ci-dessous pour référence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Clustering hiérarchique avec AgglomerativeClustering\n",
    "ca_clustering = AgglomerativeClustering(n_clusters=10)\n",
    "labels_cah = ca_clustering.fit_predict(X.toarray())\n",
    "\n",
    "# Calcul du score de silhouette pour évaluer la qualité des clusters\n",
    "silhouette_avg = silhouette_score(X.toarray(), labels_cah)\n",
    "print(f\"Score de silhouette pour le CAH : {silhouette_avg}\")\n",
    "\n",
    "# Visualisation du dendrogramme pour voir la hiérarchie des clusters\n",
    "linked = linkage(X.toarray(), 'ward')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', labels=reviews, distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title(\"Dendrogramme du clustering hiérarchique\")\n",
    "plt.xlabel(\"Index des reviews\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"lda = LatentDirichletAllocation(n_components=10, random_state=24)\n",
    "lda.fit(X)\n",
    "\n",
    "topic_values = lda.transform(X)\"\"\"\n",
    "\n",
    "lda, topic_values = joblib.load('Entrainement_modeles_tfidf/lda_tfidf.joblib')\n",
    "\n",
    "\"\"\"joblib.dump((lda, topic_values), 'Entrainement_modeles_tfidf/lda_tfidf.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['LDA_labels'] = np.argmax(topic_values, axis=1)\n",
    "\n",
    "df_clean.groupby('LDA_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                rating_Count=('rating', 'count')\n",
    "                               ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons de meilleurs résultats avec le modèle Latent Dirichlet Allocation (LDA), qui est bien adapté à notre problème de regroupement thématique des avis. En effet, LDA est conçu pour identifier des thèmes sous-jacents dans des documents textuels, ce qui permet de regrouper les avis en thématiques relativement homogènes. LDA a également permis de mettre en évidence des différences entre les groupes en termes de notes associées. Cependant, pour exploiter pleinement le potentiel de LDA, il reste essentiel d’optimiser ses hyperparamètres afin d’obtenir des clusters plus précis.\n",
    "\n",
    "HDBSCAN, bien que potentiellement adapté pour identifier des clusters de forme et de densité variées, n’a pas encore donné de résultats satisfaisants probablement en raison de réglages d’hyperparamètres sous-optimaux. HDBSCAN peut être utile pour notre problème si nous parvenons à optimiser ces paramètres, car il pourrait détecter des groupes d'avis bien distincts, notamment des groupes d’avis extrêmes (très positifs ou très négatifs) sans forcer une structure sphérique comme K-Means. Il reste néanmoins limité dans un contexte de données textuelles de grande dimension.\n",
    "\n",
    "K-Means a également été utilisé, mais cet algorithme n’est pas parfaitement adapté à notre problème. En effet, K-Means fonctionne mieux avec des clusters de forme sphérique et nécessite un calcul intensif des distances, ce qui devient coûteux en termes de mémoire et de temps d'exécution lorsque le volume de données textuelles et la dimensionnalité des vecteurs sont élevés (nous utilisons des vecteurs de dimension 1000). K-Means est moins performant pour capturer des relations complexes entre les thèmes des avis et se révèle inadapté aux données textuelles non linéaires.\n",
    "\n",
    "Un autre défi majeur est lié à l’utilisation de TF-IDF, qui présente des limites pour représenter le texte. Bien qu’il permette de transformer les avis en vecteurs, TF-IDF ne capture pas les relations contextuelles entre les mots et se limite à la fréquence de chaque mot dans les documents. Par conséquent, des mots ayant des significations similaires mais des formes différentes (par exemple, \"excellent\" et \"super\") ne sont pas traités comme liés, ce qui réduit la cohérence des clusters. De plus, TF-IDF produit des vecteurs de grande dimension, entraînant une consommation importante de mémoire sans pour autant améliorer la qualité des clusters.\n",
    "\n",
    "Pour surmonter ces limitations, nous allons explorer des méthodes plus avancées, notamment des techniques de Deep Learning telles que les embeddings de texte. Les embeddings permettent de réduire la dimensionnalité tout en capturant des relations sémantiques et contextuelles riches entre les mots. En utilisant des embeddings préentraînés ou des modèles de transformers, nous pourrons mieux représenter les avis sous forme de vecteurs de faible dimension mais significatifs, optimisant ainsi les ressources mémoire et accélérant les calculs. Cette approche devrait nous permettre d’obtenir des clusters plus cohérents et représentatifs des thèmes exprimés dans les avis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de méthode plus avancée (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement d'un modèle Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme chaque review en liste de mots\n",
    "sentences = [review.split() for review in reviews]\n",
    "\n",
    "# Initialisation d'un modèle Word2Vec\n",
    "w2v_model = Word2Vec(vector_size=200, seed=24)\n",
    "\n",
    "# Construction du vocabulaire à partir des phrases.\n",
    "w2v_model.build_vocab(sentences)\n",
    "\n",
    "# Récupération du vocabulaire\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "vocab_size = len(words)\n",
    "print(f\"Taille du vocabulaire : {vocab_size}\")\n",
    "\n",
    "# Entrainement du modèle\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_vector(review: list[str], word_vectors: dict[str, np.ndarray], model: Word2Vec) -> np.ndarray:\n",
    "    \n",
    "    \"\"\"\n",
    "    Cette fonction génère un vecteur pour chaque review en calculant la moyenne des vecteurs de mots\n",
    "    présents dans la review.\n",
    "\n",
    "    Arguments :\n",
    "    review -- Liste de mots de la revue.\n",
    "    word_vectors -- Dictionnaire contenant les représentations vectorielles des mots (généré par le modèle Word2Vec).\n",
    "    model -- Le modèle Word2Vec utilisé pour extraire la taille des vecteurs (utile si la revue n'a pas de mots valides).\n",
    "    \n",
    "    Retourne :\n",
    "    Un vecteur représentant la revue. Si aucun mot de la revue n'est présent dans le vocabulaire du modèle,\n",
    "    retourne un vecteur nul (composé de zéros).\n",
    "    \"\"\"\n",
    "    \n",
    "    vectors = [word_vectors[word] for word in review if word in word_vectors]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "# On transforme chaque review en un vecteur\n",
    "review_vectors = np.array([get_review_vector(review, word_vectors, w2v_model) for review in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pour effectuer le clustering des reviews après word embedding, j'ai choisi de prendre la moyenne des vecteurs de chaque mot pour obtenir un vecteur représentatif par review. Cette approche simplifie le processus de clustering en réduisant chaque review à un vecteur de dimension fixe, facilitant ainsi l’application d’algorithmes comme HDBSCAN. Bien qu'elle implique une perte d’information contextuelle en résumant chaque review en un seul vecteur, cette méthode est bien adaptée pour extraire les thèmes principaux de manière efficace et optimise les performances en termes de mémoire et de temps de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hdbscan_w2v = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=100)\n",
    "hdbscan_w2v.fit(review_vectors)\"\"\"\n",
    "\n",
    "hdbscan_w2v = joblib.load('Entrainement_modeles_deep/hdbscan_w2v.joblib')\n",
    "\n",
    "# Résultats des clusters\n",
    "labels_hdbscan_w2v = hdbscan_w2v.labels_\n",
    "\n",
    "calinski_score = calinski_harabasz_score(review_vectors, labels_hdbscan_w2v)\n",
    "print(\"Score de Calinski-Harabasz :\", calinski_score) \n",
    "\n",
    "\"\"\"joblib.dump(hdbscan_w2v, 'Entrainement_modeles_deep/hdbscan_w2v.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['DBSCAN_w2v_labels'] = labels_hdbscan_w2v\n",
    "\n",
    "df_clean.groupby('DBSCAN_w2v_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                          rating_Count=('rating', 'count')\n",
    "                                         ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Normalisation pour garantir que toutes les valeurs sont non négatives\n",
    "scaler = MinMaxScaler()\n",
    "review_vectors_scaled = scaler.fit_transform(review_vectors)\n",
    "\n",
    "lda_w2v = LatentDirichletAllocation(n_components=10, random_state=24)\n",
    "lda_w2v.fit(review_vectors_scaled)\n",
    "\n",
    "topic_values_w2v = lda_w2v.transform(review_vectors_scaled)\"\"\"\n",
    "\n",
    "lda_w2v, topic_values_w2v = joblib.load('Entrainement_modeles_deep/lda_w2v.joblib')\n",
    "\n",
    "\"\"\"joblib.dump((lda_w2v, topic_values_w2v), 'Entrainement_modeles_deep/lda_w2v.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['LDA_w2v_labels'] = np.argmax(topic_values_w2v, axis=1)\n",
    "\n",
    "df_clean.groupby('LDA_w2v_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                       rating_Count=('rating', 'count')\n",
    "                                      ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation d'un modèle pré entrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_pt = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des review en un vecteur\n",
    "review_vectors_pt = np.array([get_review_vector(review, w2v_model_pt, w2v_model_pt) for review in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hdbscan_w2v_pt = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=100)\n",
    "hdbscan_w2v_pt.fit(review_vectors_pt)\"\"\"\n",
    "\n",
    "hdbscan_w2v_pt = joblib.load('Entrainement_modeles_deep/hdbscan_w2v_pt.joblib')\n",
    "\n",
    "# Résultats des clusters\n",
    "labels_hdbscan_w2v_pt = hdbscan_w2v_pt.labels_\n",
    "\n",
    "calinski_score = calinski_harabasz_score(review_vectors_pt, labels_hdbscan_w2v_pt)\n",
    "print(\"Score de Calinski-Harabasz :\", calinski_score) \n",
    "\n",
    "\"\"\"joblib.dump(hdbscan_w2v_pt, 'Entrainement_modeles_deep/hdbscan_w2v_pt.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['DBSCAN_w2v_pt_labels'] = labels_hdbscan_w2v_pt\n",
    "\n",
    "df_clean.groupby('DBSCAN_w2v_pt_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                rating_Count=('rating', 'count')\n",
    "                               ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scaler = MinMaxScaler()\n",
    "review_vectors_scaled_pt = scaler.fit_transform(review_vectors_pt)\n",
    "\n",
    "lda_w2v_pt = LatentDirichletAllocation(n_components=10, random_state=24)\n",
    "lda_w2v_pt.fit(review_vectors_scaled_pt)\n",
    "\n",
    "topic_values_w2v_pt = lda_w2v_pt.transform(review_vectors_scaled_pt)\"\"\"\n",
    "\n",
    "lda_w2v_pt, topic_values_w2v_pt = joblib.load('Entrainement_modeles_deep/lda_w2v_pt.joblib')\n",
    "\n",
    "\"\"\"joblib.dump((lda_w2v_pt, topic_values_w2v_pt), 'Entrainement_modeles_deep/lda_w2v_pt.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['LDA_w2v_pt_labels'] = np.argmax(topic_values_w2v_pt, axis=1)\n",
    "\n",
    "df_clean.groupby('LDA_w2v_pt_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                          rating_Count=('rating', 'count')\n",
    "                                         ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si un GPU est disponible, on utilise CUDA pour le traitement, sinon, on revient sur le CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Chargement du tokenizer et du modèle BERT pré-entraîné\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Envoyer le modèle sur le GPU (ou CPU si le GPU n'est pas disponible)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "def process_batch(batch_reviews):\n",
    "    \"\"\"\n",
    "    Cette fonction prend un lot de reviews sous forme de liste de chaînes de caractères,\n",
    "    les tokenise, passe les tokens dans le modèle BERT, et renvoie les embeddings moyens pour chaque review.\n",
    "    \n",
    "    Paramètres :\n",
    "    - batch_reviews (list): Liste de chaînes de caractères représentant les avis (reviews).\n",
    "    \n",
    "    Retour :\n",
    "    - sentence_embeddings (Tensor): Tensor de taille (batch_size, embedding_dim) contenant les embeddings moyens\n",
    "      de chaque review dans le lot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Utilisation du tokenizer BERT pour convertir les textes en séquences de tokens    \n",
    "    inputs = bert_tokenizer(batch_reviews, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Envoyer les entrées sur le même périphérique (GPU ou CPU)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Traitement du lot avec le modèle BERT\n",
    "    # On utilise 'torch.no_grad()' pour désactiver le calcul des gradients, ce qui économise de la mémoire pendant l'inférence\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Récupération des embeddings\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Moyenne des embeddings pour obtenir un vecteur par review\n",
    "    sentence_embeddings = embeddings.mean(dim=1)\n",
    "    \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Diviser les avis en lots de taille 1000 pour éviter un problème de mémoire\n",
    "batch_size = 500\n",
    "all_embeddings = []\n",
    "\n",
    "# Diviser les données en lots de taille 'batch_size'\n",
    "for i in range(0, len(reviews), batch_size):\n",
    "    # Extraire le lot actuel d'avis\n",
    "    batch_reviews = reviews[i:i + batch_size]\n",
    "    \n",
    "    # Traiter chaque lot et récupérer les embeddings\n",
    "    batch_embeddings = process_batch(batch_reviews)\n",
    "    \n",
    "    # Ajouter les embeddings du batch au résultat final\n",
    "    all_embeddings.append(batch_embeddings)\n",
    "\n",
    "    # Libérer la mémoire GPU après chaque lot\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Concaténer les embeddings de toutes les phrases\n",
    "sentence_embeddings = torch.cat(all_embeddings, dim=0)\"\"\"\n",
    "\n",
    "sentence_embeddings = joblib.load('Entrainement_modeles_deep/embeddings_bert.joblib')\n",
    "\n",
    "# Afficher la forme des embeddings de phrases\n",
    "print(\"Shape des embeddings pour chaque review :\", sentence_embeddings.shape)\n",
    "\n",
    "\"\"\"joblib.dump(sentence_embeddings, 'Entrainement_modeles_deep/embeddings_bert.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hdbscan_bert = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=100)\n",
    "hdbscan_bert.fit(sentence_embeddings)\"\"\"\n",
    "\n",
    "hdbscan_bert = joblib.load('Entrainement_modeles_deep/hdbscan_bert.joblib')\n",
    "\n",
    "# Résultats des clusters\n",
    "labels_hdbscan_bert = hdbscan_bert.labels_\n",
    "\n",
    "calinski_score = calinski_harabasz_score(sentence_embeddings, labels_hdbscan_bert)\n",
    "print(\"Score de Calinski-Harabasz :\", calinski_score) \n",
    "\n",
    "\"\"\"joblib.dump(hdbscan_bert, 'Entrainement_modeles_deep/hdbscan_bert.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['HDBSCAN_BERT_labels'] = labels_hdbscan_bert\n",
    "\n",
    "df_clean.groupby('HDBSCAN_BERT_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                          rating_Count=('rating', 'count')\n",
    "                                         ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scaler = MinMaxScaler()\n",
    "sentence_embedding_scaled = scaler.fit_transform(sentence_embeddings)\n",
    " \n",
    "lda_bert = LatentDirichletAllocation(n_components=10, random_state=24)\n",
    "lda_bert.fit(sentence_embedding_scaled)\n",
    "\n",
    "topic_values_bert = lda_bert.transform(sentence_embedding_scaled)\"\"\"\n",
    "\n",
    "lda_bert, topic_values_bert = joblib.load('Entrainement_modeles_deep/lda_bert.joblib')\n",
    "\n",
    "\"\"\"joblib.dump((lda_bert, topic_values_bert), 'Entrainement_modeles_deep/lda_bert.joblib')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['LDA_BERT_labels'] = np.argmax(topic_values_bert, axis=1)\n",
    "\n",
    "df_clean.groupby('LDA_BERT_labels').agg(rating_mean=('rating', 'mean'),\n",
    "                                          rating_Count=('rating', 'count')\n",
    "                                         ).sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(language='french',  nr_topics=10)\n",
    "topics, proba = topic_model.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['BERTopic_labels'] = topics\n",
    "\n",
    "df_clean.groupby('BERTopic_labels').agg(\n",
    "    rating_mean=('rating', 'mean'),\n",
    "    rating_Count=('rating', 'count')\n",
    ").sort_values(by='rating_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ugc_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
